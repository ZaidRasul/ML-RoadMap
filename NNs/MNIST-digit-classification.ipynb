{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc7e7f8c",
   "metadata": {},
   "source": [
    "Using pytorch to make a NN that tells the number from handwritten image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56b39762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor #so we can convert the images to tensors\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn as nn, optim as optim\n",
    "from torch.nn import functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174dc74",
   "metadata": {},
   "source": [
    "Downloading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689edc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True, # telling it to load the training dataset\n",
    "    download = True, # to download the dataset \n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False, # telling it to load the test dataset\n",
    "    download = True, # to download the dataset \n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ec5929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data info\n",
    "\n",
    "#train_data\n",
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254722d",
   "metadata": {},
   "source": [
    "data size and image size which is 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2822106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_data.data.shape\n",
    "test_data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d1834d",
   "metadata": {},
   "source": [
    "distinct numbers from 0 to 9 which we need the nn to give"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5b844ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5, 0, 4,  ..., 5, 6, 8])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1365910d",
   "metadata": {},
   "source": [
    "Defining a data loader that will load data in batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd92c05e",
   "metadata": {},
   "source": [
    "Purpose of this is to load data in bathces instead of feeding all at once.  \n",
    "why?  \n",
    "it helps in speed and we can control the change in each epoch.  \n",
    "shuffle = true, shuffles the data before every epoch so model doesnt see data in   \n",
    "same order. which reduces bias and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de11848",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = {\n",
    "    'train': DataLoader(train_data, batch_size=100, shuffle=True, num_workers=1),\n",
    "    'test': DataLoader(test_data, batch_size=100, shuffle=True, num_workers=1)\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1de813c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x7d7c43a38890>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x7d7c43cbd640>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5f96889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        #input layer\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        #output layer\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        #flattening the input\n",
    "        x = x.view(x.size(0), 28*28)\n",
    "        #input layer relu activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        #output layer\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e0c6b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = net()\n",
    "# model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "addd75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64a10eb",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "726f1f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Loss: 0.0002\n",
      "Epoch [2/6], Loss: 0.0001\n",
      "Epoch [3/6], Loss: 0.0000\n",
      "Epoch [4/6], Loss: 0.0000\n",
      "Epoch [5/6], Loss: 0.0001\n",
      "Epoch [6/6], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # set the model to training mode\n",
    "    loss = 0.0\n",
    "    for image, label in loader['train']:\n",
    "        # forward pass\n",
    "        output = model(image)\n",
    "        # calculate loss\n",
    "        loss = criterion(output, label)\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss/len(loader['train']):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41f3f21",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "664d11e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 97.76%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # set the model to evaluation mode\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for image, label in loader['test']:\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "\n",
    "print(f\"accuracy: {100 * correct / total:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-roadmap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
